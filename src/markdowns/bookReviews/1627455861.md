# "Human Compatible: Artificial Intelligence And The Problem of Control" by Stuart Russell

***Completed Reading July 28th 2021***

General Artificial Intelligence is gonna be the next revolutionary technology to be adopted by the world. Once in its mature state, it will become ubiquitous in almost every facet of our lives. Whatever you can think of, general AI will have an accurate “answer” for it (and I put “answer” in quotes because in layman’s terms, answers usually mean correct. In the AI realm, answer means how accurate a conclusion is compared to other outcomes). This is a great step for humanity, but like with any revolutionary technology, there are always trade-offs. And in AI, those trade-offs can range from benign to devastating.

Stuart Russell starts off with why AI is still in a very “infant” state. By infancy, I mean in terms of emulating the neurological network of the human brain (I’ll dive into this a bit more later). But we’ve made astounding strides in the past couple of decades. For example, there was an AI program called AlphaGo created by DeepMind that was designed to play the game Go. The DeepMind developers used deep neural network algorithms which utilizes hierarchical decision trees to find the best move based on previous learned behavior. Basically, the algorithm learned via “watching” millions of Go games. And used this learning to create patterns. That’s why in 2016, AlphaGo was able to beat Lee Sedol whom at the time was the best Go player in the world.

Now, you might be thinking, “That’s awesome, we can utilize this same technology in the real world.” Well… not so fast. You see Go like Chess has a very rigid and confined world. Yes, you can make a massive amount of moves in Go or Chess, but there are 10^360 possible moves in Go. A lot, but doable especially with the rate that Moore’s Law is going. Now, the real world is a WHOLE another ball game. For example, for us humans, we usually have a process for getting from our house to a particular vacation spot. We find out where we’re going, then we have figure out what flight we’re taking, how to get to the airport, what are we bringing, etc… You get the point. AI, at this stage, can’t make those procedural jumps because there’s literally an infinite amount of possibilities based on each individual. And Stuart Russell came up with six characteristics of problems that influence design of AI:
1. Whether the environment is fully observable or partially observable
2. Whether the environment and actions are discrete or effectively continuous
3. Whether the environment contains the agents or not
4. Whether the environment is dynamically changing, so that the time to make decisions is tightly constrained
5. Whether the outcomes of actions are predictable or unpredictable, and whether those rules are known or unknown
6. The length of the horizon over which decision quality is measured according to the objective - this may be very short, of intermediate duration, or very long
   
And this gets to the dangers of AI. As AI becomes more and more generalized (becoming better at learning from experience), there are two major issues. The first is AI will start to make assumptions based on their “owners'” behavior. We humans are very fickle and irrational entities. One minute we prefer pizza and the next we prefer broccoli. And on top of this, we combat with our experiencing self and our remembering self. For example, Daniel Kahneman ran an experiment where subjects would put their hands in ice water for 60 seconds in the first trial. Then in the next trial, they’ll put their hands in the same temperature of ice water for 60 seconds remove their hands and then the temperature of the water will get slightly warmer and then they put their hands in the water for another 30 seconds. They asked the subjects which trial would they repeat again if they had to choose. It turns out that 80% of the subjects preferred to do the second trial over the first one. The remembering self will remember the best and worst moments of the experience even though the general experience might be worse overall. But for AI, it will interpret these experiences as the user’s preferences when in reality, this could cause a level of discomfort for the user.

The second issue is AI gaining some semblance of self awareness. Now there’s a huge debate about this, so I’m gonna just reiterate what Stuart Russell’s opinion is on this. Without any countermeasures or “feature flags” and with deep learning AI closely resembling the neurological networks of a normal human brain, then it is possible for this AI agent to gain consciousness. Combine that with AI being super intelligent since it learns magnitudes faster than we can, and we have a recipe for potential disaster. Like the atomic bombs and weapons of mass destruction, technology in the wrong hands can lead to devastating consequences.

Deep Learning AI is coming whether people like it or not. There’s too much potential in the field for countries to ignore. The best thing we can do as society is prepare and create proper security measures, so we don’t introduce something we can’t revert.
